Die gewonnenen Erkenntnisse können nun dazu genutzt werden, um Lösungen für homogene lineare Differentialgleichungssysteme zu finden.

\subsection{Diagonale Matrizen}\label{subsec:03-01}
Zunächst werden diagonale Matrizen $A = \diag\{a_1,\dots,a_n\}$ betrachtet.
Vorgegeben sei also eine Diagonalmatrix
\begin{equation*}
    A = \diag\{a_1,\dots,a_n\}
    = \begin{pmatrix}
          a_1 &        & \\
              & \ddots & \\
              &        & a_3
    \end{pmatrix}.
\end{equation*}
Für ein~\ref{eq:dgls} liegen dann die $n$ unabhängigen Differentialgleichungen
\begin{equation*}
    y'_i(x) = a_i y_i(x) \qquad (i = 1,\dots,n)
\end{equation*}
vor, deren Lösungen zu einem gegebenen Anfangswert $\vec{y}_0 = (y_{0_1},\dots,y_{0_n})^T$ durch
\begin{equation*}
    y_i(x) = \e^{a_i x} y_{0_i} \qquad (i = 1,\dots,n)
\end{equation*}
gegeben sind.
Mittels Matrixexponentialfunktion lässt sich die Lösung also durch
\begin{equation*}
    \vec{y}(x) = \e^{Ax}
    = \begin{pmatrix}
        \e^{a_1 x} &        & \\
                   & \ddots & \\
                   &        & \e^{a_n x}
    \end{pmatrix}
\end{equation*}
angeben.

\subsection{Diagonalisierbare Matrizen}\label{subsec:03-02}
Das Lösen eines~\ref{eq:dgls} mit einer diagonalisierbaren Koeffizientenmatrix $A$ kann auf den obigen Fall der~\nameref{subsec:03-01}
zurückgeführt werden.
Dazu stellen wir $A$ mit einer Diagonalmatrix $L = \diag\{\lambda_1,\dots,\lambda_n\}$ aus den Eigenwerten $\lambda_i$ ($i = 1,\dots,n$)
und $V$ die Matrix der zugehörigen Eigenvektoren von $A$ durch
\begin{equation*}
    A = V L V^{-1}
\end{equation*}
dar.

\begin{theorem}\label{thm:diagbar-cp-solution}
    Sei $A = V L V^{-1}$ eine diagonalisierbare Matrix mit $L = \diag\{\lambda_1,\dots,\lambda_n\}$,
    wobei $\lambda_i$ ($i = 1,\dots,n$) die Eigenwerte und $V$ die Matrix mit den zugehörigen Eigenvektoren von $A$ sind.
    Dann ist die Lösung eines~\hyperref[eq:cp]{C\textc{auchy}-Problems}
    \begin{gather*}
        \vec{y}'(x) = A \vec{y}(x) = V L V^{-1} \vec{y}(x), \qquad \vec{y}_0 = (y_{1_0},\dots,y_{n_0})^T\\
        \intertext{durch}
        \begin{aligned}
            \vec{y}(x)
            &= V \e^{L x} V^{-1} \vec{y}_0\\
            \big( &= \left. V \diag\left\{\e^{\lambda_1 x}, \dots,\e^{\lambda_n x}\right\} V^{-1} \right)
        \end{aligned}
    \end{gather*}
    gegeben.
\end{theorem}

\begin{proof}
    Das~\ref{eq:dgls}
    \begin{gather*}
        \vec{y}'(x) = V L V^{-1} \vec{y}(x)\\
        \intertext{lässt sich zu}
        V^{-1} \vec{y}'(x) = L V^{-1} \vec{y}(x).
    \end{gather*}
    umformen.
    Durch Substitution von $\vec{u} = V^{-1} \vec{y}$ ergibt sich das~\ref{eq:dgls}
    \begin{equation}\tag{$\triangle$}\label{eq:03-02-01}
        \vec{u}'(x) = L \vec{u}(x), \qquad \vec{u}_0 \coloneqq \vec{u}(x_0) = V^{-1} \vec{y}(x_0) = V^{-1} \vec{y}_0.
    \end{equation}
    Da $L$ eine Diagonalmatrix ist, folgt mit~\autoref{subsec:03-01}, dass
    \begin{equation*}
        \vec{u}(x) = \e^{L x} \vec{u}_0
    \end{equation*}
    die Lösung von~\eqref{eq:03-02-01} ist.
    Rücksubstitution ergibt schließlich die Lösung
    \begin{equation*}
        \vec{u}(x) = \e^{L x} \vec{u}_0
        \qquad\Leftrightarrow\qquad V^{-1} \vec{y} = \e^{L x} V^{-1} \vec{y}_0
        \qquad\Leftrightarrow\qquad \vec{y} = V \e^{L x} V^{-1} \vec{y}_0.
    \end{equation*}
\end{proof}

\begin{corollary}
    Sei $A$ diagonalisierbar mit $A = V L V^{-1}$.
    Dann gilt für die Matrixexponentialfunktion
    \begin{equation*}
        \e^{A} = V \e^{L} V^{-1}.
    \end{equation*}
\end{corollary}

\begin{proof}
    Zuerst wird gezeigt, dass $A^k = V L^k V^{-1}$ für $k \in \NN$:
    \begin{alignat*}{2}
        A^k &= (V L V^{-1})^k &&= \underbrace{(V L V^{-1}) (V L V^{-1}) (V L V^{-1}) \dots (V L V^{-1})}_{k \text{-mal}}\\
            &= V L \underbrace{(V^{-1} V)}_{E} L \underbrace{(V^{-1} V)}_{E} \dots \underbrace{(V^{-1} V)}_{E} L V^{-1} &&= V \underbrace{L \cdots L}_{k \text{-mal}} V^{-1}\\
            &= V L^k V^{-1}
    \end{alignat*}
    Damit folgt dann für die Matrixexponentialfunktion
    \begin{alignat*}{2}
        \e^{A} &= \e^{V L V^{-1}} &&= \sum^\infty_{k=0} \frac{(V L V^{-1})^k}{k!}\\
               &= \sum^\infty_{k=0} \frac{V L^k V^{-1}}{k!} &&= V \left( \sum^\infty_{k=0} \frac{L^k}{k!} \right) V^{-1}\\
               &= V e^L V^{-1}
    \end{alignat*}
\end{proof}

\hyperref[thm:diagbar-cp-solution]{Folgerung~\ref*{thm:diagbar-cp-solution}} lässt sich leicht ersichtlich auf einen allgemeinen Fall von ähnlichen Matrizen $A = S B S^{-1}$ übertragen, d.h.
\begin{equation*}
    \e^A = \e^{S B S^{-1}} = S \e^B S^{-1}.
\end{equation*}

\subsection{Nicht-diagonalisierbare Matrizen}\label{subsec:03-03}
Jede Matrix $A$ lässt sich auf eine \emph{J\textc{ordan}-Normalform} $J = V^{-1} A V$ bringen.
Eine J\textc{ordan}-Normalform $J$ ist eine Blockdiagonalmatrix der Form
\begin{gather*}
    J = \begin{pmatrix}
            J_1 &        &  \\
                & \ddots & \\
                &        & J_n
    \end{pmatrix}\\
    \intertext{mit J\textc{ordan}-Kästchen}
    J_i = \begin{pmatrix}
              \lambda & 1       &        & \\
                      & \lambda & \ddots & \\
                      &         & \ddots & 1 \\
                      &         &        & \lambda
    \end{pmatrix}
    \qquad (i = 1,\dots,n).
\end{gather*}
Das folgende \hyperref[thm:blockdiag-exp]{Lemma~\ref*{thm:blockdiag-exp}} lässt vermuten,
dass die gewonnenen Erkenntnisse aus~\autoref{subsec:03-02} anwendbar sein könnten.

\begin{lemma}\label{thm:blockdiag-exp}
    Sei $A \in \RR^{nd \times nd}$ eine Blockdiagonalmatrix
    \begin{equation*}
        A = \begin{pmatrix}
                A_1 &        & \\
                    & \ddots & \\
                    &        & A_n
        \end{pmatrix},
    \end{equation*}
    mit quadratische Matrizen $A_1, \dots, A_n \in \RR^{d \times d}$.
    Dann gilt
    \begin{equation*}
        \e^A = \begin{pmatrix}
                   \e^{A_1} &        & \\
                            & \ddots & \\
                            &        & \e^{A_n}
        \end{pmatrix}.
    \end{equation*}
\end{lemma}

\begin{proof}
    Es gilt
    \begin{equation*}
        A^k = \begin{pmatrix}
                  A^k_1   &        & \\
                          & \ddots & \\
                          &        & A^k_n
        \end{pmatrix}.
    \end{equation*}
    Damit folgt dann
    \begin{alignat*}{2}
        \e^A
        &= \sum^\infty_{k=0} \frac{A^k}{k!}
            &&= \sum^\infty_{k=0}
            \begin{pmatrix}
                \frac{A^k_1}{k!} &        & \\
                                 & \ddots & \\
                                 &        & \frac{A^k_n}{k!}
            \end{pmatrix}\\
        &= \sum^\infty_{k=0}
            \begin{pmatrix}
                \frac{A^k_1}{k!} &        & \\
                                 & \ddots & \\
                                 &        & \frac{A^k_n}{k!}
            \end{pmatrix}
            &&= \begin{pmatrix}
                    \sum^\infty_{k=0} \frac{A^k_1}{k!} &        & \\
                                                       & \ddots & \\
                                                       &        & \sum^\infty_{k=0} \frac{A^k_n}{k!}
            \end{pmatrix}\\
        &= \begin{pmatrix}
               \e^{A_1} &        & \\
                        & \ddots & \\
                        &        & \e^{A_n}
        \end{pmatrix}
    \end{alignat*}
\end{proof}

Es ergeben sich also Lösungen eines gegebenen~\hyperref[eq:cp]{C\textc{auchy}-Problems} mit einer nicht-diagonalisierbaren Matrix $A = V J V^{-1}$ durch
\begin{equation*}
    \vec{y}(x) = V \e^{J x} V^{-1} \vec{y}_0.
\end{equation*}
Zudem zeigt~\hyperref[thm:blockdiag-exp]{Lemma~\ref*{thm:blockdiag-exp}},
dass sich die Berechnung von $\e^J$ auf die einzelnen $\e^{J_i}$ für $i = 1,\dots,n$ reduzieren lässt.\\
Ein J\textc{ordan}-Kästchen $J_i$ lässt sich weiter zerlegen zu
\begin{equation*}
    J_i = \begin{pmatrix}
              \lambda   & 1       &        & \\
                        & \lambda & \ddots & \\
                        &         & \ddots & 1 \\
                        &         &        & \lambda
    \end{pmatrix}
    = \underbrace{\begin{pmatrix}
        \lambda  &        & \\
                 & \ddots & \\
                 &        & \lambda
    \end{pmatrix}}_{= L}
    + \underbrace{\begin{pmatrix}
          0 & 1 &        & \\
            & 0 & \ddots & \\
            &   & \ddots & 1 \\
            &   &        & 0
    \end{pmatrix}}_{\eqqcolon N}
    = L + N.
\end{equation*}
Dabei ist $N$ eine \emph{nilpotente} Matrix, d.h.\ es existiert ein $l \in \NN$ so, dass $N^l = 0$.
Die Matrizen $L$ und $N$ kommutieren, weshalb
\begin{equation*}
    \e^{L + N} = \e^L \e^N
\end{equation*}
gilt.
Da $L$ eine Diagonalmatrix ist, ist die Lösung von $\e^L$ bereits aus~\autoref{subsec:03-01} bekannt.
Für $N$ als nilpotente Matrix ergibt sich zudem eine endliche Summe bei der Berechnung von $\e^N$,
da ab einem $l \in \NN$ gilt, dass $N^l = N^{l+1} = \dots = 0$, d.h.
\begin{equation*}
    \e^N
    = \sum^\infty_{k=0} \frac{N^k}{k!}
    = \sum^{l-1}_{k=0} \frac{N^k}{k!}.
\end{equation*}